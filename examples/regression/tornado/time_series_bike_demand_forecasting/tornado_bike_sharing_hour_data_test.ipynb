{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f6005c-1e82-45cc-9013-fa89752acadf",
   "metadata": {
    "id": "e4f6005c-1e82-45cc-9013-fa89752acadf"
   },
   "source": [
    "# Regression using Cyclic Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee82ebe-8aa4-49ee-a84b-2a9aed8868c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8nS3cek3utK",
    "outputId": "e7b226b6-9dc5-4896-8066-8d105725fdd0"
   },
   "source": [
    "First, install the  package and its dependencies\n",
    "\n",
    "```sh\n",
    "!pip install cyclic-boosting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
   "metadata": {
    "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use the test dataset from kaggle\n",
    "\n",
    "Sign in to Kaggle at the URL below and download the dataset.  \n",
    "https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset\n",
    "\n",
    "Place the downloaded dataset in the following directory.  \n",
    "examples/regression/tornado/test_1/bike_sharing_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795a87a-df58-4b1a-8717-b95203fd65cd",
   "metadata": {},
   "source": [
    "For time-series data, a \"date\" column must be included to indicate the date and time the data was obtained. The column name and format must be consistent. The \"dayofweek\" column for the day of the week and the \"dayofyear\" column for the total number of days in the year are automatically created if not already present, but if they are already present, the column names must be correct.\n",
    "\n",
    "This dataset has hourly data. In this dataset, the \"instant\" column is the data number. The \"casual\" and \"registered\" columns are the breakdown of sales, so they should be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39b738-890f-45d0-b015-cc0a2306dc24",
   "metadata": {
    "id": "7c39b738-890f-45d0-b015-cc0a2306dc24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parpath = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "df = pd.read_csv(parpath + \"/bike_sharing_data/hour.csv\")\n",
    "df = df.rename(columns={'dteday': 'date', 'weekday': 'dayofweek'})\n",
    "df = df.drop(columns=['instant', 'casual', 'registered'])\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['date'] = df['date'] + df['hr'].map(lambda x: datetime.timedelta(hours=float(x)))\n",
    "\n",
    "df.to_csv(\"./bike_sharing_hour.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8692944-7695-4823-82ef-fb2a22a399a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220e50d",
   "metadata": {},
   "source": [
    "# Automated Machine Learning with Tornado\n",
    "With tornado, you can automatically perform data preparation, feature property setting, hyperparameter tuning, model building, training, evaluation, and plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting.tornado import Generator, Manager, Trainer\n",
    "\n",
    "data_deliverler = Generator.TornadoDataModule(\"./bike_sharing_hour.csv\")\n",
    "manager = Manager.TornadoVariableSelectionModule()\n",
    "trainer = Trainer.SqueezeTrainer(data_deliverler, manager)\n",
    "trainer.run(target=\"cnt\", log_policy=\"compute_COD\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15833e8f",
   "metadata": {},
   "source": [
    "# Load the best model and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea2396",
   "metadata": {},
   "source": [
    "Get the best model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "model_nos = []\n",
    "for p in sorted(Path(\"./models/\").glob(\"model*\")):\n",
    "    model_nos.append(str(p)[str(p).find(\"_\") + 1 :])\n",
    "model_path = f\"./models/model_{model_nos[-1]}/model_{model_nos[-1]}.pkl\"\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a0e00",
   "metadata": {},
   "source": [
    "Make predictions with the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'season': [4],\n",
    "    'yr': [0],\n",
    "    'mnth': [11],\n",
    "    'hr': [18],\n",
    "    'holiday': [0],\n",
    "    'workingday': [1],\n",
    "    'weathersit': [2],\n",
    "    'temp': [0.341667],\n",
    "    'atemp': [0.323221],\n",
    "    'hum': [0.575833],\n",
    "    'windspeed': [0.305362],\n",
    "    'dayofweek': [4],\n",
    "    'dayofyear': [180],\n",
    "}\n",
    "X = pd.DataFrame(data)\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    CB_est = pickle.load(f)\n",
    "    yhat = CB_est.predict(X.copy())\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy comparison with base Cyclic boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting import flags, common_smoothers, observers\n",
    "from cyclic_boosting.pipelines import pipeline_CBPoissonRegressor\n",
    "\n",
    "from cyclic_boosting.smoothing.onedim import SeasonalSmoother\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
    "train, validation = train_test_split(df, test_size=0.2, random_state=0)\n",
    "y = np.asarray(train[\"cnt\"])\n",
    "X = train.drop(columns=\"cnt\")\n",
    "y_val = np.asarray(validation[\"cnt\"])\n",
    "X_val = validation.drop(columns=\"cnt\")\n",
    "if not all(np.asarray(train['cnt']) == manager.y):\n",
    "    raise ValueError(\"Accuracy comparison is not available because\\n\"\n",
    "                     \"the data split is not the same as that of tornado.\")\n",
    "\n",
    "feature_properties = {\n",
    "    \"season\": flags.IS_UNORDERED,\n",
    "    \"dayofweek\": flags.IS_ORDERED,\n",
    "    \"dayofyear\": flags.IS_CONTINUOUS | flags.IS_LINEAR,\n",
    "    \"yr\": flags.IS_ORDERED,\n",
    "    \"mnth\": flags.IS_ORDERED,\n",
    "    \"hr\": flags.IS_ORDERED,\n",
    "    \"holiday\": flags.IS_UNORDERED,\n",
    "    \"workingday\": flags.IS_UNORDERED,\n",
    "    \"weathersit\": flags.IS_UNORDERED,\n",
    "    \"temp\": flags.IS_CONTINUOUS,\n",
    "    \"atemp\": flags.IS_CONTINUOUS,\n",
    "    \"hum\": flags.IS_CONTINUOUS,\n",
    "    \"windspeed\": flags.IS_CONTINUOUS,\n",
    "}\n",
    "\n",
    "features = [\n",
    "    \"season\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"yr\",\n",
    "    \"mnth\",\n",
    "    \"hr\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "    \"weathersit\",\n",
    "    \"temp\",\n",
    "    \"atemp\",\n",
    "    \"hum\",\n",
    "    \"windspeed\",\n",
    "]\n",
    "\n",
    "explicit_smoothers = {\n",
    "    (\"dayofyear\",): SeasonalSmoother(order=3),\n",
    "}\n",
    "\n",
    "plobs = [\n",
    "    observers.PlottingObserver(iteration=1),\n",
    "    observers.PlottingObserver(iteration=-1),\n",
    "]\n",
    "\n",
    "CB_est = pipeline_CBPoissonRegressor(\n",
    "    feature_properties=feature_properties,\n",
    "    feature_groups=features,\n",
    "    observers=plobs,\n",
    "    maximal_iterations=50,\n",
    "    smoother_choice=common_smoothers.SmootherChoiceGroupBy(\n",
    "        use_regression_type=True,\n",
    "        use_normalization=False,\n",
    "        explicit_smoothers=explicit_smoothers,\n",
    "    ),\n",
    ")\n",
    "\n",
    "_ = CB_est.fit(X.copy(), y)\n",
    "\n",
    "metrics_path = f\"./models/model_{model_nos[-1]}/metrics_{model_nos[-1]}.txt\"\n",
    "\n",
    "with open(metrics_path, \"rb\") as f:\n",
    "    metrics_tornado = f.read().decode(\"utf-8\")\n",
    "    mse_tornado = float([x for x in metrics_tornado.split(\"\\n\") if \"MSE\" in x][0].split(\":\")[1])\n",
    "    mae_tornado = float([x for x in metrics_tornado.split(\"\\n\") if \"MAE\" in x][0].split(\":\")[1])\n",
    "    wmape_tornado = float([x for x in metrics_tornado.split(\"\\n\") if \"WMAPE\" in x][0].split(\":\")[1])\n",
    "\n",
    "\n",
    "yhat = CB_est.predict(X_val.copy())\n",
    "mse = np.nanmean(np.square(y_val - yhat))\n",
    "mae = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "val_results = pd.DataFrame([[np.sqrt(mse_tornado), mae_tornado, wmape_tornado],\n",
    "                            [np.sqrt(mse), mae, wmape]],\n",
    "                           columns=[\"RMSE\", \"MAE\", \"WMAPE\"],\n",
    "                           index=[\"CB_tornado\", \"Base CB\"])\n",
    "print(val_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
