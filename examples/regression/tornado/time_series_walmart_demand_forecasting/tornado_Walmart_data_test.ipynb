{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f6005c-1e82-45cc-9013-fa89752acadf",
   "metadata": {
    "id": "e4f6005c-1e82-45cc-9013-fa89752acadf"
   },
   "source": [
    "# Regression using Cyclic Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee82ebe-8aa4-49ee-a84b-2a9aed8868c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8nS3cek3utK",
    "outputId": "e7b226b6-9dc5-4896-8066-8d105725fdd0"
   },
   "source": [
    "First, install the  package and its dependencies\n",
    "\n",
    "```sh\n",
    "!pip install cyclic-boosting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
   "metadata": {
    "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851ae6a",
   "metadata": {},
   "source": [
    "# Let's use the test dataset from kaggle\n",
    "\n",
    "Sign in to Kaggle at the URL below and download the dataset.  \n",
    "https://www.kaggle.com/datasets/yasserh/walmart-dataset\n",
    "\n",
    "Place the downloaded dataset in the following directory.  \n",
    "examples/regression/tornado/time_series_Walmart_demand_forecasting/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795a87a-df58-4b1a-8717-b95203fd65cd",
   "metadata": {},
   "source": [
    "For time-series data, a \"date\" column must be included to indicate the date and time the data was obtained. The column name and format must be consistent. The \"dayofweek\" column for the day of the week and the \"dayofyear\" column for the total number of days in the year are automatically created if not already present, but if they are already present, the column names must be correct.\n",
    "\n",
    "This dataset has data for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Walmart.csv\")\n",
    "df = df.rename(columns={'Date': 'date'})\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format='%d-%m-%Y')\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=0)\n",
    "df_train.to_csv(\"./Walmart_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220e50d",
   "metadata": {},
   "source": [
    "# Automated Machine Learning with Tornado\n",
    "With tornado, you can automatically perform data preparation, feature property setting, hyperparameter tuning, model building, training, evaluation, and plotting! (but, It might take a few minutes. Have a coffee break during execution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting.tornado import Generator, Manager, Tornado\n",
    "\n",
    "data_deliverler = Generator.TornadoDataModule(\"./Walmart_train.csv\")\n",
    "manager = Manager.TornadoManager()\n",
    "predictor = Tornado.InteractionSearchModel(data_deliverler, manager)\n",
    "predictor.fit(target=\"weekly_sales\", criterion=\"COD\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea2396",
   "metadata": {},
   "source": [
    "Tornado model is able to point estimation and probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean point estimation\n",
    "yhat = predictor.predict(df_test)\n",
    "print(yhat[0])\n",
    "\n",
    "# probability estimation with poisson distribution\n",
    "proba = predictor.predict_proba(df_test.head(5), output=\"pmf\")\n",
    "proba.loc[0, :].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935289",
   "metadata": {},
   "source": [
    "# Accuracy comparison with plain Cyclic boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting import flags, common_smoothers, observers\n",
    "from cyclic_boosting.pipelines import pipeline_CBPoissonRegressor\n",
    "from cyclic_boosting.smoothing.onedim import SeasonalSmoother\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import poisson\n",
    "from cyclic_boosting.tornado.trainer.metrics import probability_distribution_accuracy\n",
    "\n",
    "\n",
    "df_train[\"dayofweek\"] = df_train[\"date\"].dt.dayofweek\n",
    "df_train[\"dayofyear\"] = df_train[\"date\"].dt.dayofyear\n",
    "df_test[\"dayofweek\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"dayofyear\"] = df_test[\"date\"].dt.dayofyear\n",
    "train, _ = train_test_split(df_train, test_size=0.2, random_state=0)\n",
    "y_train = np.asarray(train[\"Weekly_Sales\"])\n",
    "X_train = train.drop(columns=\"Weekly_Sales\")\n",
    "y_val = np.asarray(df_test[\"Weekly_Sales\"])\n",
    "X_val = df_test.drop(columns=\"Weekly_Sales\")\n",
    "if not all(np.asarray(train['Weekly_Sales']) == manager.y):\n",
    "    raise ValueError(\"Accuracy comparison is not available because\\n\"\n",
    "                     \"the data split is not the same as that of tornado.\")\n",
    "\n",
    "feature_properties = {\n",
    "    \"Store\": flags.IS_UNORDERED,\n",
    "    \"dayofweek\": flags.IS_ORDERED,\n",
    "    \"dayofyear\": flags.IS_CONTINUOUS | flags.IS_LINEAR,\n",
    "    \"Holiday_Flag\": flags.IS_UNORDERED,\n",
    "    \"Temperature\": flags.IS_CONTINUOUS,\n",
    "    \"Fuel_Price\": flags.IS_CONTINUOUS,\n",
    "    \"CPI\": flags.IS_CONTINUOUS,\n",
    "    \"Unemployment\": flags.IS_CONTINUOUS,\n",
    "}\n",
    "\n",
    "features = [\n",
    "    \"Store\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"Holiday_Flag\",\n",
    "    \"Temperature\",\n",
    "    \"Fuel_Price\",\n",
    "    \"CPI\",\n",
    "    \"Unemployment\",\n",
    "]\n",
    "\n",
    "explicit_smoothers = {\n",
    "    (\"dayofyear\",): SeasonalSmoother(order=3),\n",
    "}\n",
    "\n",
    "plobs = [\n",
    "    observers.PlottingObserver(iteration=1),\n",
    "    observers.PlottingObserver(iteration=-1),\n",
    "]\n",
    "\n",
    "CB_est = pipeline_CBPoissonRegressor(\n",
    "    feature_properties=feature_properties,\n",
    "    feature_groups=features,\n",
    "    observers=plobs,\n",
    "    maximal_iterations=50,\n",
    "    smoother_choice=common_smoothers.SmootherChoiceGroupBy(\n",
    "        use_regression_type=True,\n",
    "        use_normalization=False,\n",
    "        explicit_smoothers=explicit_smoothers,\n",
    "    ),\n",
    ")\n",
    "\n",
    "_ = CB_est.fit(X_train.copy(), y_train)\n",
    "\n",
    "yhat = predictor.predict(df_test)\n",
    "mse_tornado = np.nanmean(np.square(y_val - yhat))\n",
    "mae_tornado = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape_tornado = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val)\n",
    "\n",
    "yhat_pd = predictor.predict_proba(df_test, output=\"func\")\n",
    "acc_pd_tornado = probability_distribution_accuracy(y_val, yhat_pd)\n",
    "\n",
    "yhat = CB_est.predict(X_val.copy())\n",
    "mse = np.nanmean(np.square(y_val - yhat))\n",
    "mae = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val)\n",
    "\n",
    "yhat_pd = list()\n",
    "for mu in yhat:\n",
    "    yhat_pd.append(poisson(mu))\n",
    "acc_pd = probability_distribution_accuracy(y_val, yhat_pd)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "val_results = pd.DataFrame([[np.sqrt(mse_tornado), mae_tornado, wmape_tornado, acc_pd_tornado],\n",
    "                            [np.sqrt(mse), mae, wmape, acc_pd]],\n",
    "                           columns=[\"RMSE\", \"MAE\", \"WMAPE\", \"PD_ACC\"],\n",
    "                           index=[\"CB_tornado\", \"Plain CB\"])\n",
    "print(val_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee783f",
   "metadata": {},
   "source": [
    "# Accuracy comparison with LightGBM\n",
    "Please install lightgbm package before running.  \n",
    "LightGBM is an excellent approach for nonlinear forecasting when sufficient data are available, and Tornado can achieve accuracy close to LightGBM by searching for interactions while maintaining excellent explanatory power through probability distribution outputs and factor visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84af97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.drop(columns=\"date\").copy()\n",
    "X_val_ = X_val.drop(columns=\"date\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d43bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "y_train_ = np.log(y_train)\n",
    "lgb_train = lgb.Dataset(X_train_, y_train_)\n",
    "model = lgb.train(params=params,\n",
    "                    train_set=lgb_train,\n",
    "                    num_boost_round=500)\n",
    "\n",
    "yhat = np.exp(model.predict(X_val_))\n",
    "\n",
    "mse = np.nanmean(np.square(y_val - yhat))\n",
    "mae = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "val_results = pd.DataFrame([[np.sqrt(mse_tornado), mae_tornado, wmape_tornado],\n",
    "                            [np.sqrt(mse), mae, wmape]],\n",
    "                           columns=[\"RMSE\", \"MAE\", \"WMAPE\"],\n",
    "                           index=[\"CB_tornado\", \"LightGBM\"])\n",
    "print(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eff530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
