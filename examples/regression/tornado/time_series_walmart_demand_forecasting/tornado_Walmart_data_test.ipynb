{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f6005c-1e82-45cc-9013-fa89752acadf",
   "metadata": {
    "id": "e4f6005c-1e82-45cc-9013-fa89752acadf"
   },
   "source": [
    "# Regression using Cyclic Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee82ebe-8aa4-49ee-a84b-2a9aed8868c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8nS3cek3utK",
    "outputId": "e7b226b6-9dc5-4896-8066-8d105725fdd0"
   },
   "source": [
    "First, install the  package and its dependencies\n",
    "\n",
    "```sh\n",
    "!pip install cyclic-boosting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4bef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
   "metadata": {
    "id": "d032f099-abc3-414f-957d-bf915cd2bd4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851ae6a",
   "metadata": {},
   "source": [
    "# Let's use the test dataset from kaggle\n",
    "\n",
    "Sign in to Kaggle at the URL below and download the dataset.  \n",
    "https://www.kaggle.com/datasets/yasserh/walmart-dataset\n",
    "\n",
    "Place the downloaded dataset in the following directory.  \n",
    "examples/regression/tornado/time_series_Walmart_demand_forecasting/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795a87a-df58-4b1a-8717-b95203fd65cd",
   "metadata": {},
   "source": [
    "For time-series data, a \"date\" column must be included to indicate the date and time the data was obtained. The column name and format must be consistent. The \"dayofweek\" column for the day of the week and the \"dayofyear\" column for the total number of days in the year are automatically created if not already present, but if they are already present, the column names must be correct.\n",
    "\n",
    "This dataset has data for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbffc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Walmart.csv\")\n",
    "df = df.rename(columns={\"Date\": \"date\"})\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, shuffle=False)\n",
    "df_train.to_csv(\"./Walmart_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220e50d",
   "metadata": {},
   "source": [
    "# Automated Machine Learning with Tornado\n",
    "With tornado, you can automatically perform data preparation, feature property setting, hyperparameter tuning, model building, training, evaluation, and plotting! (but, It might take a few minutes. Have a coffee break during execution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting.tornado import Generator, Manager, Tornado\n",
    "\n",
    "data_deliverer = Generator.TornadoDataModule(\"./Walmart_train.csv\")\n",
    "manager = Manager.PriorPredForwardSelectionManager(is_time_series=True, combination=2, dist=\"nbinom\")\n",
    "predictor = Tornado.PriorPredInteractionSearchModel(data_deliverer, manager)\n",
    "predictor.fit(target=\"weekly_sales\", criterion=\"COD\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea2396",
   "metadata": {},
   "source": [
    "Tornado model is able to point estimation and probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean point estimation\n",
    "yhat = predictor.predict(df_test)\n",
    "print(yhat[0])\n",
    "\n",
    "# probability estimation with negative binomial distribution\n",
    "proba = predictor.predict_proba(df_test.head(5), output=\"pmf\")\n",
    "proba.loc[0, :].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935289",
   "metadata": {},
   "source": [
    "# Accuracy comparison with plain Cyclic boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclic_boosting import flags, common_smoothers, observers\n",
    "from cyclic_boosting.pipelines import pipeline_CBPoissonRegressor\n",
    "from cyclic_boosting.smoothing.onedim import SeasonalSmoother\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import poisson\n",
    "from cyclic_boosting.tornado.trainer.metrics import probability_distribution_accuracy\n",
    "\n",
    "\n",
    "df_train[\"dayofweek\"] = df_train[\"date\"].dt.dayofweek\n",
    "df_train[\"dayofyear\"] = df_train[\"date\"].dt.dayofyear\n",
    "df_test[\"dayofweek\"] = df_test[\"date\"].dt.dayofweek\n",
    "df_test[\"dayofyear\"] = df_test[\"date\"].dt.dayofyear\n",
    "train, _ = train_test_split(df_train, test_size=0.2, shuffle=False)\n",
    "y_train = np.asarray(train[\"Weekly_Sales\"])\n",
    "X_train = train.drop(columns=\"Weekly_Sales\")\n",
    "y_val = np.asarray(df_test[\"Weekly_Sales\"])\n",
    "X_val = df_test.drop(columns=\"Weekly_Sales\")\n",
    "if not all(np.asarray(train[\"Weekly_Sales\"]) == manager.y):\n",
    "    raise ValueError(\n",
    "        \"Accuracy comparison is not available because\\n\" \"the data split is not the same as that of tornado.\"\n",
    "    )\n",
    "\n",
    "feature_properties = {\n",
    "    \"Store\": flags.IS_UNORDERED,\n",
    "    \"dayofweek\": flags.IS_ORDERED,\n",
    "    \"dayofyear\": flags.IS_CONTINUOUS | flags.IS_LINEAR,\n",
    "    \"Holiday_Flag\": flags.IS_UNORDERED,\n",
    "    \"Temperature\": flags.IS_CONTINUOUS,\n",
    "    \"Fuel_Price\": flags.IS_CONTINUOUS,\n",
    "    \"CPI\": flags.IS_CONTINUOUS,\n",
    "    \"Unemployment\": flags.IS_CONTINUOUS,\n",
    "}\n",
    "\n",
    "features = [\n",
    "    \"Store\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"Holiday_Flag\",\n",
    "    \"Temperature\",\n",
    "    \"Fuel_Price\",\n",
    "    \"CPI\",\n",
    "    \"Unemployment\",\n",
    "]\n",
    "\n",
    "explicit_smoothers = {\n",
    "    (\"dayofyear\",): SeasonalSmoother(order=3),\n",
    "}\n",
    "\n",
    "plobs = [\n",
    "    observers.PlottingObserver(iteration=1),\n",
    "    observers.PlottingObserver(iteration=-1),\n",
    "]\n",
    "\n",
    "CB_est = pipeline_CBPoissonRegressor(\n",
    "    feature_properties=feature_properties,\n",
    "    feature_groups=features,\n",
    "    observers=plobs,\n",
    "    maximal_iterations=50,\n",
    "    smoother_choice=common_smoothers.SmootherChoiceGroupBy(\n",
    "        use_regression_type=True,\n",
    "        use_normalization=False,\n",
    "        explicit_smoothers=explicit_smoothers,\n",
    "    ),\n",
    ")\n",
    "\n",
    "_ = CB_est.fit(X_train.copy(), y_train)\n",
    "\n",
    "yhat = predictor.predict(df_test)\n",
    "mse_tornado = np.nanmean(np.square(y_val - yhat))\n",
    "mae_tornado = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape_tornado = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val * y_val)\n",
    "\n",
    "yhat_pd = predictor.predict_proba(df_test, output=\"func\")\n",
    "acc_pd_tornado = probability_distribution_accuracy(y_val, yhat_pd)\n",
    "\n",
    "yhat = CB_est.predict(X_val.copy())\n",
    "mse = np.nanmean(np.square(y_val - yhat))\n",
    "mae = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val * y_val)\n",
    "\n",
    "yhat_pd = list()\n",
    "for mu in yhat:\n",
    "    yhat_pd.append(poisson(mu))\n",
    "acc_pd = probability_distribution_accuracy(y_val, yhat_pd)\n",
    "\n",
    "pd.options.display.float_format = \"{:.5}\".format\n",
    "val_results = pd.DataFrame(\n",
    "    [[np.sqrt(mse_tornado), mae_tornado, wmape_tornado, acc_pd_tornado], [np.sqrt(mse), mae, wmape, acc_pd]],\n",
    "    columns=[\"RMSE\", \"MAE\", \"WMAPE\", \"PD_ACC\"],\n",
    "    index=[\"CB_tornado\", \"Plain CB\"],\n",
    ")\n",
    "print(val_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee783f",
   "metadata": {},
   "source": [
    "# Accuracy comparison with LightGBM\n",
    "Install the lightgbm package before running.  \n",
    "LightGBM is an excellent approach to non-linear forecasting when sufficient data are available; Tornado addresses the non-linear forecasting task, a problem that linear models cannot easily predict, by automatically searching for interaction terms. By default, it searches for combinations of two variables and can interpret the features captured by the model using plot_analysis. Note that higher-order combinations can also be explored, but complex models may be difficult to interpret while fitting the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67b4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c84af97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.drop(columns=\"date\").copy()\n",
    "X_val_ = X_val.drop(columns=\"date\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d43bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}\n",
    "\n",
    "y_train_ = np.log(y_train)\n",
    "lgb_train = lgb.Dataset(X_train_, y_train_)\n",
    "model = lgb.train(params=params, train_set=lgb_train, num_boost_round=500)\n",
    "\n",
    "yhat = np.exp(model.predict(X_val_))\n",
    "\n",
    "mse = np.nanmean(np.square(y_val - yhat))\n",
    "mae = np.nanmean(np.abs(y_val - yhat))\n",
    "wmape = np.nansum(np.abs(y_val - yhat) * y_val) / np.nansum(y_val * y_val)\n",
    "\n",
    "pd.options.display.float_format = \"{:.5f}\".format\n",
    "val_results = pd.DataFrame(\n",
    "    [[np.sqrt(mse_tornado), mae_tornado, wmape_tornado], [np.sqrt(mse), mae, wmape]],\n",
    "    columns=[\"RMSE\", \"MAE\", \"WMAPE\"],\n",
    "    index=[\"CB_tornado\", \"LightGBM\"],\n",
    ")\n",
    "print(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905500c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
